{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1OnafQX5lazCpbre-MWciCeDj9w6kXHHe",
      "authorship_tag": "ABX9TyPOuo1aBOBcLDCTvyOI/xCq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaimegonzalezc/KC-NLP-Practica/blob/main/PracticaNLP-2-Preprocesado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Práctica Natural Language Processing - KeepCoding\n",
        "\n",
        "## 2. Preprocesado del texto\n",
        "Ahora vamos a procesar el texto para poder pasar a la etapa de entrenamiento con los datos lo más limpios posibles.\n",
        "\n",
        "Se va a realizar un conjunto de funciones que realicen una modificación en el texto introducido para después agruparlas en una función unica de procesado para pasar por los modelos de análisis de sentimiento.\n",
        "\n",
        "Por ello, cargamos librerías y vamos función a función realizando fases del procesado:"
      ],
      "metadata": {
        "id": "vT6AvSekxNNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "data = pd.read_json('/content/drive/MyDrive/Bootcamp IA/NLP/Práctica/Digital_Music_5.json', lines=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogJrTe2oyr2F",
        "outputId": "b679b836-c3e5-4c91-8d23-a562f45dea62"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En primer lugar se van a realizar las funciones que no requieren de tokenización, para una vez limpiado el texto, tokenizarlo y proceder con las siguientes fases del procesado.\n",
        "\n",
        "### Signos de puntuación"
      ],
      "metadata": {
        "id": "DRLeIKXe9buw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eliminar_signos_puntuacion(texto):\n",
        "    return texto.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "texto = \"Hi, world!\"\n",
        "print(eliminar_signos_puntuacion(texto))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaHVkB1u9hGp",
        "outputId": "706dc351-ee51-4bc7-9a47-5df1793395eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eliminación de números\n",
        "En las reviews podrían encontrarse números que no aportan nada en el análisis de sentimiento y pueden dar problemas al vectorizar las palabras."
      ],
      "metadata": {
        "id": "hze1itBdAnNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eliminar_numeros(texto):\n",
        "    return re.sub(r'\\d+', '', texto)\n",
        "\n",
        "texto = \"There are 3 balls in this bag.\"\n",
        "print(eliminar_numeros(texto))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zupg7BxXA6t-",
        "outputId": "e00a7cda-5797-4166-c5bb-ecb7bc963586"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are  balls in this bag.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eliminación de URLs\n",
        "Tras revisar los datos, se ha encontrado que en algunos comentarios la gente deja URLs para apoyar su valoración, por lo que también se debe eliminar utilizando para ello regex."
      ],
      "metadata": {
        "id": "0WCbcODlBh-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eliminar_urls_menciones(texto):\n",
        "    texto = re.sub(r'http\\S+|www\\S+|https\\S+', '', texto, flags=re.MULTILINE)\n",
        "    texto = re.sub(r'\\@\\w+|\\#','', texto)\n",
        "    return texto\n",
        "\n",
        "texto = \"Great product! It works perfectly and arrived on time. Check it out here: https://www.amazon.com/dp/B07QK2SPP7.\"\n",
        "print(eliminar_urls_menciones(texto))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NK07G687BZTz",
        "outputId": "3c29a094-c654-4030-a9bc-46fdd0b1fe4c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Great product! It works perfectly and arrived on time. Check it out here: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Borrado de Stopwords\n",
        "Para eliminar los Stopwords, ya debemos introducir la información tokenizada, por lo que antes de comprobar la función tokenizaremos el texto de entrada."
      ],
      "metadata": {
        "id": "1WSKiCsM-aVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eliminar_stopwords(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "texto = \"This is an example sentence with some stopwords in it\"\n",
        "tokens = word_tokenize(texto)\n",
        "print(eliminar_stopwords(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpnpnhI8-Zjr",
        "outputId": "22507271-259f-4670-de3c-10103cf363e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'example', 'sentence', 'stopwords']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lematizado de tokens\n",
        "Lematizamos las palabras para extraer el lema de cada palabra y agrupar palabras de la misma familia."
      ],
      "metadata": {
        "id": "I-__5YPXEtKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lematizar_palabras(tokens):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "texto = \"The children are playing in the parks and they will be running tomorrow\"\n",
        "tokens = word_tokenize(texto)\n",
        "print(lematizar_palabras(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ5IcUd8Es36",
        "outputId": "04e39df6-8425-43b6-cffd-b54f1c08bbea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'child', 'are', 'playing', 'in', 'the', 'park', 'and', 'they', 'will', 'be', 'running', 'tomorrow']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Función general\n",
        "Por último, vamos a generar esa función final y a probarla con nuestro dataset."
      ],
      "metadata": {
        "id": "n0eAeeraGGQg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV7I--BMxHj0",
        "outputId": "1d6213e0-b398-4ddc-c180-9293204ee57e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          reviewText  \\\n",
            "0  It's hard to believe \"Memory of Trees\" came ou...   \n",
            "1  A clasically-styled and introverted album, Mem...   \n",
            "2  I never thought Enya would reach the sublime h...   \n",
            "3  This is the third review of an irish album I w...   \n",
            "4  Enya, despite being a successful recording art...   \n",
            "\n",
            "                                reviewText_procesado  \n",
            "0  hard believe memory tree came year agoit held ...  \n",
            "1  clasicallystyled introverted album memory tree...  \n",
            "2  never thought enya would reach sublime height ...  \n",
            "3  third review irish album write today others cr...  \n",
            "4  enya despite successful recording artist doesn...  \n"
          ]
        }
      ],
      "source": [
        "def preprocesar_texto(texto):\n",
        "    texto = texto.lower() # Convertir a minúsculas\n",
        "    texto = eliminar_signos_puntuacion(texto)\n",
        "    texto = eliminar_numeros(texto)\n",
        "    texto = eliminar_urls_menciones(texto)\n",
        "    tokens = word_tokenize(texto)\n",
        "    tokens = eliminar_stopwords(tokens)\n",
        "    tokens = lematizar_palabras(tokens)\n",
        "    texto_procesado = ' '.join(tokens) # Volvemos a unir los tokens en texto\n",
        "    return texto_procesado\n",
        "\n",
        "# Aplicar la función de preprocesamiento a la columna de texto\n",
        "data['reviewText_procesado'] = data['reviewText'].apply(preprocesar_texto)\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame procesado para comparar\n",
        "print(data[['reviewText', 'reviewText_procesado']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para acabar, vamos a guardar los datos procesados para trabajar con ellos en el entramiento de los modelos:"
      ],
      "metadata": {
        "id": "cG5Lckb7NVNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('/content/drive/MyDrive/Bootcamp IA/NLP/Práctica/reviews_preprocesadas.csv', index=False)"
      ],
      "metadata": {
        "id": "3GDcjogpL40W"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}